{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import l5kit.data\n",
    "import numpy as np\n",
    "import torch\n",
    "import zarr\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset\n",
    "from l5kit.evaluation import (\n",
    "    read_gt_csv,\n",
    ")\n",
    "from l5kit.geometry import transform_point, transform_points\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "import utils\n",
    "from fast_rasteriser import build_custom_rasterizer\n",
    "\n",
    "logger = logging.getLogger(\"dataset\")\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = config.L5KIT_DATA_FOLDER\n",
    "\n",
    "\n",
    "def fix_agent_state(agent_data, agent_state):\n",
    "    if agent_state is not None:\n",
    "        if math.cos(agent_data[\"yaw\"] - agent_state[\"yaw\"]) < -0.5 and agent_state[\"velocity\"] < 0:\n",
    "            agent_state[\"yaw\"] += math.pi\n",
    "            agent_state[\"velocity\"] *= -1\n",
    "            agent_state[\"accel\"] *= -1\n",
    "\n",
    "    return agent_data, agent_state\n",
    "\n",
    "\n",
    "class LyftDataset(torch.utils.data.Dataset):\n",
    "    DSET_TRAIN = \"train\"\n",
    "    DSET_TRAIN_XXL = \"train_XXL\"\n",
    "    DSET_VALIDATION = \"val\"\n",
    "    DSET_VALIDATION_CHOPPED = \"val_chopped\"\n",
    "    DSET_TEST = \"test\"\n",
    "\n",
    "    # for backward compatibility\n",
    "    STAGE_TRAIN = \"train\"\n",
    "    STAGE_VALIDATION = \"val\"\n",
    "    STAGE_VALIDATION_CHOPPED = \"val_chopped\"\n",
    "    STAGE_TEST = \"test\"\n",
    "\n",
    "    name_2_dataloader_key = {\n",
    "        DSET_TRAIN: \"train_data_loader\",\n",
    "        DSET_TRAIN_XXL: \"train_data_loader\",\n",
    "        DSET_VALIDATION: \"val_data_loader\",\n",
    "        DSET_VALIDATION_CHOPPED: \"val_data_loader\",\n",
    "        DSET_TEST: \"test_data_loader\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dset_name=None,\n",
    "            cfg_path=\"./agent_motion_config.yaml\",\n",
    "            cfg_data=None,\n",
    "            stage=None,\n",
    "    ):\n",
    "        print(f\"Initializing LyftDataset {dset_name}...\")\n",
    "        if stage is not None:\n",
    "            print('DDEPRECATION WARNING! LyftDataset:: argument \"stage=\" is deprecated, use \"dset_name=\" instead')\n",
    "            if dset_name is None:\n",
    "                dset_name = stage\n",
    "            else:\n",
    "                raise ValueError('LyftDataset::Please use only \"dset_name\" argument')\n",
    "        assert dset_name is not None\n",
    "        self.dm = LocalDataManager(None)\n",
    "        self.dset_name = dset_name\n",
    "        if cfg_data is None:\n",
    "            self.cfg = utils.DotDict(load_config_data(cfg_path))\n",
    "        else:\n",
    "            self.cfg = utils.DotDict(cfg_data)\n",
    "\n",
    "        self.dset_cfg = self.cfg[LyftDataset.name_2_dataloader_key[dset_name]].copy()\n",
    "\n",
    "        if self.cfg[\"raster_params\"][\"map_type\"] == \"py_satellite\":\n",
    "            print(\"WARNING! USING SLOW RASTERIZER!!! py_satellite\")\n",
    "            self.rasterizer = build_rasterizer(self.cfg, self.dm)\n",
    "        self.rasterizer = build_custom_rasterizer(self.cfg, self.dm)\n",
    "\n",
    "        if dset_name == LyftDataset.DSET_VALIDATION_CHOPPED:\n",
    "            eval_base_path = Path(\"/opt/data3/lyft_motion_prediction/prediction_dataset/scenes/validate_chopped_100\")\n",
    "            eval_zarr_path = str(Path(eval_base_path) / Path(self.dm.require(self.dset_cfg[\"key\"])).name)\n",
    "            eval_mask_path = str(Path(eval_base_path) / \"mask.npz\")\n",
    "            self.eval_gt_path = str(Path(eval_base_path) / \"gt.csv\")\n",
    "            self.zarr_dataset = ChunkedDataset(eval_zarr_path).open(cached=False)\n",
    "            self.agent_dataset = AgentDataset(\n",
    "                self.cfg,\n",
    "                self.zarr_dataset,\n",
    "                self.rasterizer,\n",
    "                agents_mask=np.load(eval_mask_path)[\"arr_0\"],\n",
    "            )\n",
    "\n",
    "            self.val_chopped_gt = defaultdict(dict)\n",
    "            for el in read_gt_csv(self.eval_gt_path):\n",
    "                self.val_chopped_gt[el[\"track_id\"] + el[\"timestamp\"]] = el\n",
    "        elif dset_name == LyftDataset.DSET_TEST:\n",
    "            self.zarr_dataset = ChunkedDataset(self.dm.require(self.dset_cfg[\"key\"])).open(cached=False)\n",
    "            test_mask = np.load(f\"{config.L5KIT_DATA_FOLDER}/scenes/mask.npz\")[\"arr_0\"]\n",
    "            self.agent_dataset = AgentDataset(self.cfg, self.zarr_dataset, self.rasterizer, agents_mask=test_mask)\n",
    "        else:\n",
    "            zarr_path = self.dm.require(self.dset_cfg[\"key\"])\n",
    "            print(f\"Opening Chunked Dataset {zarr_path}...\")\n",
    "            self.zarr_dataset = ChunkedDataset(zarr_path).open(cached=False)\n",
    "            print(\"Creating Agent Dataset...\")\n",
    "            self.agent_dataset = AgentDataset(\n",
    "                self.cfg,\n",
    "                self.zarr_dataset,\n",
    "                self.rasterizer,\n",
    "                min_frame_history=0,\n",
    "                min_frame_future=10,\n",
    "            )\n",
    "            print(\"Creating Agent Dataset... [OK]\")\n",
    "\n",
    "        if dset_name == LyftDataset.DSET_VALIDATION:\n",
    "            mask_frame100 = np.zeros(shape=self.agent_dataset.agents_mask.shape, dtype=np.bool)\n",
    "            for scene in self.agent_dataset.dataset.scenes:\n",
    "                frame_interval = scene[\"frame_index_interval\"]\n",
    "                agent_index_interval = self.agent_dataset.dataset.frames[frame_interval[0] + 99][\"agent_index_interval\"]\n",
    "                mask_frame100[agent_index_interval[0]: agent_index_interval[1]] = True\n",
    "\n",
    "            prev_agents_num = np.sum(self.agent_dataset.agents_mask)\n",
    "            self.agent_dataset.agents_mask = self.agent_dataset.agents_mask * mask_frame100\n",
    "            print(f\"nb agent: orig {prev_agents_num} filtered {np.sum(self.agent_dataset.agents_mask)}\")\n",
    "            # store the valid agents indexes\n",
    "            self.agent_dataset.agents_indices = np.nonzero(self.agent_dataset.agents_mask)[0]\n",
    "\n",
    "        self.w, self.h = self.cfg[\"raster_params\"][\"raster_size\"]\n",
    "\n",
    "        self.add_agent_state = self.cfg[\"model_params\"][\"add_agent_state\"]\n",
    "        self.agent_state = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.agent_dataset)\n",
    "\n",
    "    def __getitem__(self, item_idx):\n",
    "        data = self.agent_dataset[item_idx]\n",
    "        np.savez(\"~/artem/ddata.npz\", data)\n",
    "        return data\n",
    "\n",
    "\n",
    "def pos_ahead(agent_data):\n",
    "    time_ahead = 2.5\n",
    "    distance_ahead = 5.0\n",
    "\n",
    "    xy = agent_data[\"centroid\"]\n",
    "    vel = agent_data[\"velocity\"]\n",
    "    return xy + vel * time_ahead\n",
    "\n",
    "\n",
    "class TLColor(Enum):\n",
    "    unknown = -1\n",
    "    green = 0\n",
    "    yellow = 1\n",
    "    red = 2\n",
    "\n",
    "    green_left = 3\n",
    "    green_right = 4\n",
    "    yellow_left = 5\n",
    "    yellow_right = 6\n",
    "    red_left = 7\n",
    "    red_right = 8\n",
    "\n",
    "\n",
    "class LyftDatasetPrerendered(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dset_name=None,\n",
    "            cfg_path=\"./agent_motion_config.yaml\",\n",
    "            cfg_data=None,\n",
    "            stage=None,\n",
    "    ):\n",
    "        if stage is not None:\n",
    "            print('LyftDatasetPrerendered:: argument \"stage=\" is deprecated, use \"dset_name=\" instead')\n",
    "            if dset_name is None:\n",
    "                dset_name = stage\n",
    "            else:\n",
    "                raise ValueError('LyftDatasetPrerendered::Please use only \"dset_name\" argument')\n",
    "        assert dset_name is not None\n",
    "        logger.info(f\"Initializing prerendered {dset_name} dataset...\")\n",
    "        self.dm = LocalDataManager(None)\n",
    "        self.dset_name = dset_name\n",
    "        if cfg_data is None:\n",
    "            self.cfg = load_config_data(cfg_path)\n",
    "        else:\n",
    "            self.cfg = cfg_data\n",
    "\n",
    "        # only used for rgb visualisation\n",
    "        self.rasterizer = build_custom_rasterizer(self.cfg, self.dm)\n",
    "\n",
    "        self.dset_cfg = self.cfg[LyftDataset.name_2_dataloader_key[dset_name]].copy()\n",
    "\n",
    "        root_dir = self.dset_cfg.get(\"root_dir\", None)\n",
    "        if root_dir is None:\n",
    "            data_dir_name = {\n",
    "                LyftDataset.DSET_TRAIN: \"train_uncompressed\",\n",
    "                LyftDataset.DSET_TRAIN_XXL: \"train_XXL\",\n",
    "                LyftDataset.DSET_VALIDATION: \"validate_uncompressed\",\n",
    "                LyftDataset.DSET_TEST: \"test\",\n",
    "            }[dset_name]\n",
    "            self.root_dir_name = join(\n",
    "                config.L5KIT_DATA_FOLDER, self.cfg[\"raster_params\"][\"pre_render_cache_dir\"], data_dir_name\n",
    "            )\n",
    "        else:\n",
    "            self.root_dir_name = join(config.L5KIT_DATA_FOLDER, root_dir)\n",
    "        print(\"load pre-rendered raster from\", self.root_dir_name)\n",
    "\n",
    "        self.segmentation_output = self.cfg[\"raster_params\"].get(\"segmentation_output\", None)\n",
    "        self.segmentation_results_dir = self.cfg[\"raster_params\"].get(\"segmentation_results_dir\", None)\n",
    "        self.add_own_agent_mask = self.cfg[\"raster_params\"].get(\"add_own_agent_mask\", False)\n",
    "        print(f\"Segmentation model res: {self.segmentation_results_dir} {self.add_own_agent_mask}\")\n",
    "\n",
    "        all_files_fn = join(self.root_dir_name, self.dset_cfg.get(\"filepaths_cache\", \"all_files\") + \".npy\")\n",
    "        try:\n",
    "            logger.info(f\"Loading cached filenames from {all_files_fn}\")\n",
    "            self.all_files = np.load(all_files_fn, allow_pickle=True)\n",
    "        except FileNotFoundError:\n",
    "            logger.info(f\"Generating and caching filenames in {all_files_fn}\")\n",
    "            self.all_files = list(sorted(glob.glob(f\"{self.root_dir_name}/**/*.npz\", recursive=True)))\n",
    "            print(f\"Generated all npz paths and saved to {all_files_fn}\")\n",
    "            np.save(all_files_fn, self.all_files)\n",
    "        print(f\"Found {len(self.all_files)} agents\")\n",
    "        self.add_agent_state = self.cfg[\"model_params\"][\"add_agent_state\"]\n",
    "        self.add_agent_state_history = self.cfg[\"model_params\"].get(\"add_agent_state_history\", False)\n",
    "        self.agent_state_history_steps = self.cfg[\"model_params\"].get(\"agent_state_history_steps\", 20)\n",
    "        self.max_agent_in_state_history = self.cfg[\"model_params\"].get(\"max_agent_in_state_history\", 16)\n",
    "        self.w, self.h = self.cfg[\"raster_params\"][\"raster_size\"]\n",
    "\n",
    "        self.tf_face_colors = {}\n",
    "\n",
    "        zarr_path = self.dm.require(self.dset_cfg[\"key\"])\n",
    "        print(f\"Opening Chunked Dataset {zarr_path}...\")\n",
    "        # print(\"Creating Agent Dataset...\")\n",
    "        # self.agent_dataset = AgentDataset(\n",
    "        #     self.cfg,\n",
    "        #     self.zarr_dataset,\n",
    "        #     self.rasterizer,\n",
    "        #     min_frame_history=0,\n",
    "        #     min_frame_future=10,\n",
    "        # )\n",
    "\n",
    "        if self.add_agent_state_history:\n",
    "            self.zarr_dataset = ChunkedDataset(zarr_path).open()\n",
    "            self.all_scenes = self.zarr_dataset.scenes[:].copy()\n",
    "            self.all_frames_agent_interval = self.zarr_dataset.frames['agent_index_interval'].copy()\n",
    "        print(\"Creating Agent Dataset... [OK]\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def tl_element_color(self, element):\n",
    "        if not element.element.HasField(\"traffic_control_element\"):\n",
    "            return TLColor.unknown\n",
    "\n",
    "        traffic_el = element.element.traffic_control_element\n",
    "\n",
    "        if traffic_el.HasField(f\"signal_red_face\"):\n",
    "            return TLColor.red\n",
    "\n",
    "        if traffic_el.HasField(f\"signal_left_arrow_red_face\"):\n",
    "            return TLColor.red_left\n",
    "        if traffic_el.HasField(f\"signal_upper_left_arrow_red_face\"):\n",
    "            return TLColor.red_left\n",
    "\n",
    "        if traffic_el.HasField(f\"signal_right_arrow_red_face\"):\n",
    "            return TLColor.red_right\n",
    "        if traffic_el.HasField(f\"signal_upper_right_arrow_red_face\"):\n",
    "            return TLColor.red_right\n",
    "\n",
    "        if traffic_el.HasField(f\"signal_yellow_face\"):\n",
    "            return TLColor.yellow\n",
    "\n",
    "        if traffic_el.HasField(f\"signal_left_arrow_yellow_face\"):\n",
    "            return TLColor.yellow_left\n",
    "        if traffic_el.HasField(f\"signal_upper_left_arrow_yellow_face\"):\n",
    "            return TLColor.yellow_left\n",
    "\n",
    "        if traffic_el.HasField(f\"signal_right_arrow_yellow_face\"):\n",
    "            return TLColor.yellow_right\n",
    "        if traffic_el.HasField(f\"signal_upper_right_arrow_yellow_face\"):\n",
    "            return TLColor.yellow_right\n",
    "\n",
    "        if traffic_el.HasField(f\"signal_green_face\"):\n",
    "            return TLColor.green\n",
    "\n",
    "        if traffic_el.HasField(f\"signal_left_arrow_green_face\"):\n",
    "            return TLColor.green_left\n",
    "        if traffic_el.HasField(f\"signal_upper_left_arrow_green_face\"):\n",
    "            return TLColor.green_left\n",
    "\n",
    "        if traffic_el.HasField(f\"signal_right_arrow_green_face\"):\n",
    "            return TLColor.green_right\n",
    "        if traffic_el.HasField(f\"signal_upper_right_arrow_green_face\"):\n",
    "            return TLColor.green_right\n",
    "\n",
    "        return TLColor.unknown\n",
    "\n",
    "    def tf_face_color(self, tl_id) -> TLColor:\n",
    "        if tl_id not in self.tf_face_colors:\n",
    "            proto_API = self.rasterizer.sat_rast.proto_API\n",
    "            # tl_colour = TLColor.unknown\n",
    "            # if proto_API.is_traffic_face_colour(tl_id, \"red\"):\n",
    "            #     tl_colour = TLColor.red\n",
    "            # elif proto_API.is_traffic_face_colour(tl_id, \"green\"):\n",
    "            #     tl_colour = TLColor.green\n",
    "            # elif proto_API.is_traffic_face_colour(tl_id, \"yellow\"):\n",
    "            #     tl_colour = TLColor.yellow\n",
    "            # self.tf_face_colors[tl_id] = tl_colour\n",
    "            self.tf_face_colors[tl_id] = self.tl_element_color(proto_API[tl_id])\n",
    "\n",
    "        return self.tf_face_colors[tl_id]\n",
    "\n",
    "    def __getitem__(self, item_idx):\n",
    "        fn = self.all_files[item_idx]\n",
    "        data = np.load(fn, allow_pickle=True)\n",
    "\n",
    "        agent_id = data[\"agent_id\"].item()\n",
    "        # non_masked_frame_agents = data[\"non_masked_frame_agents\"].item()\n",
    "        # agent_data, agent_state = fix_agent_state(*non_masked_frame_agents[agent_id])\n",
    "        # state_vec = generate_state_vec(self.cfg, data, agent_data, agent_state, item_idx, agent_id)\n",
    "        agent_from_world = data[\"agent_from_world\"]\n",
    "\n",
    "        state_vec = None\n",
    "\n",
    "        if \"tl_lanes_masks4\" in data:\n",
    "            image = np.concatenate((data[\"image_box\"], data[\"image_semantic\"]), axis=2).transpose(2, 0, 1)\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "\n",
    "            if self.cfg[\"model_params\"].get(\"nb_raster4_channels\", 0) > 0:\n",
    "                tl_masks = data[\"tl_lanes_masks4\"].item()\n",
    "                tl_history = data[\"history_tl_faces\"]\n",
    "                tl4_steps = 3  # now, 1 sec ago, 2 sec ago\n",
    "                nb_tl4_colors = 9\n",
    "                nb_tl4_colors_categories = 2  # known off, known on\n",
    "                image_tl4 = np.zeros(\n",
    "                    (tl4_steps * nb_tl4_colors * nb_tl4_colors_categories, image.shape[1] // 4, image.shape[2] // 4),\n",
    "                    dtype=np.float32)\n",
    "                for tl_delay_id, tl_delay_frame in enumerate([0, 10, 20]):\n",
    "                    if tl_delay_frame >= len(tl_history):\n",
    "                        break\n",
    "                    for known_tl in tl_history[tl_delay_frame]:\n",
    "                        face_id = known_tl['face_id']\n",
    "                        traffic_light_face_status = known_tl['traffic_light_face_status']\n",
    "                        if traffic_light_face_status[\n",
    "                            2] < 0.5 and face_id in tl_masks:  # skip unknown and outside of raster\n",
    "                            color_code = self.tf_face_color(face_id)\n",
    "                            if color_code != TLColor.unknown:\n",
    "                                output_plane_id = (\n",
    "                                                              tl_delay_id * nb_tl4_colors + color_code.value) * nb_tl4_colors_categories\n",
    "                                # output_value = traffic_light_face_status[0] - 0.2 * traffic_light_face_status[1]\n",
    "                                image_tl4[output_plane_id, tl_masks[face_id]] = traffic_light_face_status[0]\n",
    "                                image_tl4[output_plane_id + 1, tl_masks[face_id]] = traffic_light_face_status[1]\n",
    "            else:\n",
    "                image_tl4 = np.array([0.0])\n",
    "        else:\n",
    "            image = data[\"image\"].astype(np.float32) / 255.0\n",
    "            image_tl4 = np.array([0.0])\n",
    "\n",
    "        fn_relative = os.path.relpath(fn, self.root_dir_name)\n",
    "\n",
    "        other_agents_masks = None\n",
    "\n",
    "        res = {\n",
    "            k: data[k]\n",
    "            for k in [\n",
    "                \"target_availabilities\",\n",
    "                \"target_positions\",\n",
    "                \"world_from_agent\",\n",
    "                \"world_to_image\",\n",
    "                \"raster_from_world\",\n",
    "                \"raster_from_agent\",\n",
    "                \"agent_from_world\",\n",
    "                \"centroid\",\n",
    "                \"timestamp\",\n",
    "                \"track_id\",\n",
    "            ]\n",
    "        }\n",
    "        res[\"item_idx\"] = item_idx\n",
    "\n",
    "        # res[\"image_blocks_positions_agent\"] = image_blocks_positions_agent.astype(np.float32)\n",
    "        # res[\"corners\"] = corners\n",
    "        res[\"fn\"] = fn\n",
    "        res[\"fn_rel\"] = fn_relative\n",
    "        res[\"image\"] = image\n",
    "        res[\"image_4x\"] = image_tl4\n",
    "\n",
    "        if other_agents_masks is not None:\n",
    "            res[\"other_agents_masks\"] = other_agents_masks\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "def build_dataset(cfg, stage):\n",
    "    \"\"\"\n",
    "    Build dataset.\n",
    "    if several datasets are defined\n",
    "    in the dict cfg.*_data_loader.datasets then create ConcatDataset\n",
    "    \"\"\"\n",
    "    assert stage in [\"train\", \"val\", \"test\"]\n",
    "    key = LyftDataset.name_2_dataloader_key[stage]\n",
    "    cfg = cfg.copy()\n",
    "    dset_cfg = cfg[key]\n",
    "\n",
    "    if \"datasets\" in dset_cfg:\n",
    "        datasets = []\n",
    "        for dset_name, params in dset_cfg.datasets.items():\n",
    "            cur_cfg = cfg.copy()\n",
    "            # we take only the subconfig with the corresponding name!\n",
    "            OmegaConf.set_struct(cur_cfg, False)\n",
    "            cur_cfg[key].update(params)\n",
    "            OmegaConf.set_struct(cur_cfg, True)\n",
    "            if cur_cfg[key].prerendered:\n",
    "                dset_class = LyftDatasetPrerendered\n",
    "            else:\n",
    "                dset_class = LyftDataset\n",
    "            datasets.append(dset_class(dset_name, cfg_data=cur_cfg))\n",
    "        if len(datasets) > 1:\n",
    "            return ConcatDataset(datasets)\n",
    "        else:\n",
    "            return datasets[0]\n",
    "    else:\n",
    "        if dset_cfg.prerendered:\n",
    "            dset_class = LyftDatasetPrerendered\n",
    "        else:\n",
    "            dset_class = LyftDataset\n",
    "        return dset_class(dset_cfg.dset_name, cfg_data=cfg)\n",
    "\n",
    "\n",
    "def uncompress_zar(fn_src, fn_dst):\n",
    "    print(fn_src)\n",
    "    print(fn_dst)\n",
    "    print(zarr.storage.default_compressor)\n",
    "    zarr.storage.default_compressor = None\n",
    "    ds = ChunkedDataset(fn_src).open(cached=False)\n",
    "\n",
    "    dst_dataset = ChunkedDataset(fn_dst)\n",
    "    dst_dataset.initialize()\n",
    "    #     'w',\n",
    "    #     # num_scenes=len(ds.scenes),\n",
    "    #     # num_frames=len(ds.frames),\n",
    "    #     # num_agents=len(ds.agents),\n",
    "    #     # num_tl_faces=len(ds.tl_faces)\n",
    "    # )\n",
    "\n",
    "    with utils.timeit_context(\"copy scenes\"):\n",
    "        dst_dataset.scenes.append(ds.scenes[:])\n",
    "    with utils.timeit_context(\"copy frames\"):\n",
    "        dst_dataset.frames.append(ds.frames[:])\n",
    "    with utils.timeit_context(\"copy agents\"):\n",
    "        for i in tqdm(range(0, len(ds.agents), 1024 * 1024)):\n",
    "            dst_dataset.agents.append(ds.agents[i: i + 1024 * 1024])\n",
    "    with utils.timeit_context(\"copy tl_faces\"):\n",
    "        dst_dataset.tl_faces.append(ds.tl_faces[:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lyft_dataset(dset_name, zarr_name):\n",
    "    ds = dataset.LyftDataset(\n",
    "        dset_name=dset_name,\n",
    "        cfg_data=dict(\n",
    "            raster_params=dict(\n",
    "                raster_size=[224, 224],\n",
    "                pixel_size=[0.5, 0.5],\n",
    "                ego_center=[0.25, 0.5],\n",
    "                map_type=\"box_semantic_fast\",\n",
    "                satellite_map_key=\"aerial_map/aerial_map.png\",\n",
    "                semantic_map_key=\"semantic_map/semantic_map.pb\",\n",
    "                dataset_meta_key=\"meta.json\",\n",
    "                filter_agents_threshold=0.5,\n",
    "                disable_traffic_light_faces=False,\n",
    "                set_origin_to_bottom=True,\n",
    "            ),\n",
    "            train_data_loader=dict(key=f\"scenes/{zarr_name}.zarr\"),\n",
    "            val_data_loader=dict(key=f\"scenes/{zarr_name}.zarr\"),\n",
    "            test_data_loader=dict(key=f\"scenes/{zarr_name}.zarr\"),\n",
    "            model_params=dict(\n",
    "                history_num_frames=20,  # used to retrive appropriate slices of history data from dataset, but only indices form history_box_frames will be rendered by box_semantic_fast rasterizer\n",
    "                history_step_size=1,\n",
    "                history_delta_time=0.1,\n",
    "                history_box_frames=[0, 1, 2, 4, 8],\n",
    "                future_num_frames=50,\n",
    "                future_step_size=1,\n",
    "                future_delta_time=0.1,\n",
    "                step_time=0.1,\n",
    "                add_agent_state=False,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import numcodecs\n",
    "import glob\n",
    "\n",
    "from l5kit.data import LocalDataManager\n",
    "\n",
    "import config\n",
    "import dataset\n",
    "import utils\n",
    "\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = config.L5KIT_DATA_FOLDER\n",
    "DST_DIR = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LyftDataset train_XXL...\n",
      "Opening Chunked Dataset /home/stepankonev/ll5/data/scenes/train_XXL.zarr...\n",
      "Creating Agent Dataset...\n",
      "Creating Agent Dataset... [OK]\n"
     ]
    }
   ],
   "source": [
    "ds = create_lyft_dataset(\"train_XXL\", \"train_XXL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAY\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/artem/data.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5b6a0f0f87c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/artem/l5kit/l5kit/l5kit/dataset/agent.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mstate_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_index\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscene_index\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YAY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/artem/data.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavez\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavez\u001b[0;34m(file, *args, **kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \"\"\"\n\u001b[0;32m--> 616\u001b[0;31m     \u001b[0m_savez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_savez\u001b[0;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZIP_STORED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     \u001b[0mzipf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mzipfile_factory\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allowZip64'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/artem/data.npz'"
     ]
    }
   ],
   "source": [
    "ds.agent_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
